{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE8cX2Y-wfWA"
   },
   "source": [
    "# Import Libraries and set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7nidurC0l3l"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import random as python_random\n",
    "import os\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import ResNet50, InceptionResNetV2\n",
    "\n",
    "from keras import models, layers\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for results folder (create if doesn't exist)\n",
    "base_folder = './results'\n",
    "if not os.path.exists('./results'):\n",
    "    os.mkdir('./results')\n",
    "\n",
    "subfolders = ['mc', 'histories', 'acc_loss_plots', 'roc_auc', 'cm', 'cls_rpt']\n",
    "\n",
    "for sf in subfolders:\n",
    "    filepath = '{}/{}'.format(base_folder, subfolders)\n",
    "    if not os.path.exists(filepath):\n",
    "        os.mkdir(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDIkn8tC0l3k"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lhhGWXq0l3o"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./meta.csv')\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=0, stratify=df['gender'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=0, stratify=train_df['gender'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgrhIfr90l37"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(train_df, val_df, test_df, img_size, batch_size):\n",
    "  # data generator for training data\n",
    "  train_generator = train_datagen.flow_from_dataframe(train_df, x_col=\"path\", \n",
    "                                                      y_col=\"gender\", \n",
    "                                                      target_size=(img_size,img_size), \n",
    "                                                      batch_size=batch_size, \n",
    "                                                      class_mode='binary')\n",
    "\n",
    "  # data generator for validation data\n",
    "  validation_generator = validation_datagen.flow_from_dataframe(val_df, x_col=\"path\", \n",
    "                                                                y_col=\"gender\", \n",
    "                                                                target_size=(img_size,img_size),\n",
    "                                                                batch_size=batch_size, \n",
    "                                                                class_mode='binary')\n",
    "\n",
    "  # data generator for validation data to be used for prediction\n",
    "  predict_generator = validation_datagen.flow_from_dataframe(val_df, x_col=\"path\", \n",
    "                                                             y_col=\"gender\", \n",
    "                                                             target_size=(img_size,img_size),\n",
    "                                                             batch_size=batch_size, \n",
    "                                                             class_mode='binary',\n",
    "                                                             shuffle = False)\n",
    "\n",
    "  # data generator for testing data\n",
    "  test_generator = test_datagen.flow_from_dataframe(test_df, x_col=\"path\", \n",
    "                                                    y_col=\"gender\", \n",
    "                                                    target_size=(img_size,img_size),\n",
    "                                                    batch_size=batch_size, \n",
    "                                                    class_mode='binary',\n",
    "                                                    shuffle = False)\n",
    "  \n",
    "  generators = {'train_gen': train_generator,\n",
    "                'validation_gen': validation_generator,\n",
    "                'test_gen': test_generator,\n",
    "                'predict_gen': predict_generator}\n",
    "  \n",
    "  return generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Checkpoint callback\n",
    "def mc(title, save_path):\n",
    "    file_path = '{}/mc/{}_mc.h5'.format(save_path, title)\n",
    "    return ModelCheckpoint(file_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1Cc3ktn0l3-"
   },
   "outputs": [],
   "source": [
    "# Define EarlyStopping callback\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(history, save_path, title):\n",
    "    # Saving history to file\n",
    "    filename = '{}/histories/{}_hist.pickle'.format(save_path, title)\n",
    "    with open(filename, 'wb') as file_pi:\n",
    "        pickle.dump(history, file_pi)\n",
    "    print(\"Successfully saved history file at {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, epochs, title, generator, es, save_path):\n",
    "    train_gen = generator['train_gen']\n",
    "    validation_gen = generator['validation_gen']\n",
    "    history = model.fit(train_gen, \n",
    "                      steps_per_epoch=train_gen.samples/train_gen.batch_size, \n",
    "                      epochs=epochs,\n",
    "                      validation_data=validation_gen,\n",
    "                      validation_steps=validation_gen.samples/validation_gen.batch_size,\n",
    "                      verbose=2,\n",
    "                      callbacks=[es, mc(title, save_path)])\n",
    "\n",
    "    save_history(history.history, save_path, title)\n",
    "\n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis(model, generator, title, save_path):\n",
    "    print('Results for {}'.format(title))\n",
    "    test_gen = generator['predict_gen']\n",
    "    prediction = model.predict(test_gen,\n",
    "                             steps=test_gen.samples/test_gen.batch_size,\n",
    "                             verbose=2)\n",
    "    predicted_classes = prediction.flatten()\n",
    "    predicted_classes[predicted_classes>=0.5] = 1\n",
    "    predicted_classes[predicted_classes<0.5] = 0\n",
    "\n",
    "    actual = test_gen.classes\n",
    "    errors = np.where(predicted_classes != actual)[0]\n",
    "    print(\"Error rate {}\".format(len(errors)/test_gen.samples))\n",
    "\n",
    "    genders = ['Female', 'Male']\n",
    "    cm = confusion_matrix(actual, predicted_classes)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt=\"d\",\n",
    "              xticklabels=genders, yticklabels=genders)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('{}/cm/{}_cm.pdf'.format(save_path, title))\n",
    "    plt.close()\n",
    "\n",
    "    print(classification_report(actual, predicted_classes, target_names=genders))\n",
    "    clsf_report = pd.DataFrame(classification_report(actual, predicted_classes, target_names=genders, output_dict=True)).transpose()\n",
    "    clsf_report.to_csv('{}/cls_rpt/{}_cr.csv'.format(save_path, title), index= True)\n",
    "\n",
    "    # Get AUC ROC Score and save as a file\n",
    "    roc_auc = roc_auc_score(actual, predicted_classes)\n",
    "    roc_auc_file = '{}/roc_auc/{}_roc_auc'.format(save_path, title)\n",
    "    params = {'title': title,\n",
    "            'roc_auc': roc_auc}\n",
    "    with open(roc_auc_file, 'w') as fp:\n",
    "    json.dump(params, fp)\n",
    "    print(\"Successfully saved auc_roc file at {}\".format(roc_auc_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_plot(history, title, save_path=None):\n",
    "    pylab.figure(figsize=(12,4))\n",
    "    pylab.subplot(1,2,1)\n",
    "    if 'acc' in history.keys():\n",
    "        pylab.plot(history['acc'], label='train')\n",
    "        pylab.plot(history['val_acc'], label='validation')\n",
    "    else:\n",
    "        pylab.plot(history['accuracy'], label='train')\n",
    "        pylab.plot(history['val_accuracy'], label='validation')\n",
    "    pylab.title('Model Accuracy for {}'.format(title))\n",
    "    pylab.xlabel('epochs')\n",
    "    pylab.ylabel('accuracy')\n",
    "    pylab.legend(loc='best')\n",
    "\n",
    "    pylab.subplot(1,2,2)\n",
    "    pylab.plot(history['loss'], label='train')\n",
    "    pylab.plot(history['val_loss'], label='validation')\n",
    "    pylab.title('Model Loss for {}'.format(title))\n",
    "    pylab.xlabel('epochs')\n",
    "    pylab.ylabel('loss')\n",
    "    pylab.legend(loc='best')\n",
    "\n",
    "    if save_path:\n",
    "        pylab.savefig('{}/acc_loss_plots/{}.pdf'.format(save_path, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_opt_plot(hist_dict, title, save_path=None):\n",
    "    pylab.figure(figsize=(12,4))\n",
    "    pylab.subplot(1,2,1)\n",
    "    for key, hist in hist_dict.items():\n",
    "        if 'acc' in hist_dict[title].keys():\n",
    "            pylab.plot(hist['val_acc'], label=key)\n",
    "        else:\n",
    "            # key is in format of e.g. inceptionresnetv2_bs64 so split key and take index 1 value\n",
    "            pylab.plot(hist['val_accuracy'], label=key.split('_')[1])\n",
    "    # title is in the format of e.g. inceptionresnetv2_bs\n",
    "    pylab.title('Validation Accuracy for {}'.format(title))\n",
    "    pylab.xlabel('epochs')\n",
    "    pylab.ylabel('accuracy')\n",
    "    pylab.legend(loc='best')\n",
    "\n",
    "    pylab.subplot(1,2,2)\n",
    "    for key, hist in hist_dict.items():\n",
    "        pylab.plot(hist['val_loss'], label=key.split('_')[1])\n",
    "    pylab.title('Validation Loss for {}'.format(title))\n",
    "    pylab.xlabel('epochs')\n",
    "    pylab.ylabel('loss')\n",
    "    pylab.legend(loc='best')\n",
    "\n",
    "    if save_path:\n",
    "    pylab.savefig('{}/acc_loss_plots/{}.pdf'.format(save_path, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GCR4HzB0l4I"
   },
   "source": [
    "# Train with base model set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcQ6ANy5yfhk"
   },
   "outputs": [],
   "source": [
    "# Base Model Parameters\n",
    "IMG_SIZE_1 = 224\n",
    "IMG_SIZE_2 = 299\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_NEURONS = 1024\n",
    "DROPOUT_RATE = 0.5\n",
    "MOMENTUM = 0.9\n",
    "OPTIMISER = 'SGD'\n",
    "SAVE_PATH = base_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xFM_FG5EvAr",
    "outputId": "ff54c7b6-9068-4e3a-eb9f-329c4d02afb8"
   },
   "outputs": [],
   "source": [
    "# Prepare data for base models\n",
    "generators_224 = generate_data(train_df, val_df, test_df, IMG_SIZE_1, BATCH_SIZE)\n",
    "generators_299 = generate_data(train_df, val_df, test_df, IMG_SIZE_2, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x9TRwZOyzZU"
   },
   "source": [
    "## ResNet50 Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBAKN1EmzILp"
   },
   "source": [
    "### Set Parameters for Base ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vfZzvh3Gy1lk",
    "outputId": "7840ef61-eb81-44fd-a788-182e06a854ef"
   },
   "outputs": [],
   "source": [
    "# Parameters for Base ResNet50\n",
    "TITLE = 'resnet50_base'\n",
    "START_LAYER = 143 # Layer to start unfreezing from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxmwcW_fzbSK"
   },
   "source": [
    "### Create Model for Base ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaNU4I2a0l4I"
   },
   "outputs": [],
   "source": [
    "# Download ResNet50 weights\n",
    "resnet50_conv = ResNet50(weights='imagenet', include_top=False, input_tensor=None, input_shape=(224,224,3))\n",
    "\n",
    "# freeze all layers except stage 4\n",
    "for layer in resnet50_conv.layers[:START_LAYER]:\n",
    "    layer.trainable = False\n",
    "for layer in resnet50_conv.layers[START_LAYER:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Build base model\n",
    "resnet50_base = models.Sequential()\n",
    "resnet50_base.add(resnet50_conv)    \n",
    "resnet50_base.add(layers.Flatten())   \n",
    "resnet50_base.add(layers.Dense(NUM_NEURONS, activation='relu'))\n",
    "resnet50_base.add(layers.Dropout(DROPOUT_RATE))\n",
    "resnet50_base.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile base model\n",
    "resnet50_base.compile(loss='binary_crossentropy', \n",
    "                      optimizer=SGD(lr=LEARNING_RATE, momentum=MOMENTUM),\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEQhQvUDz2wx"
   },
   "source": [
    "### Train (Fit) Base ResNet50 & save information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 913
    },
    "id": "CNF26IczxHPM",
    "outputId": "47da1475-7515-4372-d0c2-8416e4317a62"
   },
   "outputs": [],
   "source": [
    "# Train and save the history of training + params of model\n",
    "resnet50_base_hist = fit(resnet50_base, EPOCHS, TITLE, generators_224, es, SAVE_PATH)\n",
    "\n",
    "# Do Model Analysis + Prediction\n",
    "model_analysis(resnet50_base, generators_224, TITLE, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simple accuracy/loss graphs\n",
    "base_model_plot(resnet50_base_hist, TITLE, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxzME8AD1_b5"
   },
   "source": [
    "## InceptionResNet Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCblf8_22Jt4"
   },
   "source": [
    "### Set Parameters for Base InceptionResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxnVjIMW2Jt5",
    "outputId": "3eeb8da5-f744-4005-83a7-7245443f91fa"
   },
   "outputs": [],
   "source": [
    "# Parameters for Base InceptionResNet\n",
    "TITLE = 'inceptionresnetv2_base'\n",
    "START_LAYER = 759\n",
    "LAST_LAYER = 780"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyrtXsEH2Jt9"
   },
   "source": [
    "### Create Model for Base InceptionResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNKl__Fc2Jt-"
   },
   "outputs": [],
   "source": [
    "# Download InceptionResNet weights\n",
    "inceptionresnetv2_conv = InceptionResNetV2(weights='imagenet', include_top=False, input_tensor=None, input_shape=(299,299,3))\n",
    "\n",
    "# freeze all layers except last block\n",
    "for layer in inceptionresnetv2_conv.layers[:START_LAYER]:\n",
    "    layer.trainable = False\n",
    "for layer in inceptionresnetv2_conv.layers[START_LAYER:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Build base model\n",
    "inceptionresnetv2_base = models.Sequential()\n",
    "inceptionresnetv2_base.add(inceptionresnetv2_conv)             \n",
    "inceptionresnetv2_base.add(layers.Flatten())          \n",
    "inceptionresnetv2_base.add(layers.Dense(NUM_NEURONS, activation='relu'))\n",
    "inceptionresnetv2_base.add(layers.Dropout(DROPOUT_RATE))\n",
    "inceptionresnetv2_base.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile base model\n",
    "inceptionresnetv2_base.compile(loss='binary_crossentropy', \n",
    "                     optimizer=SGD(lr=LEARNING_RATE, momentum=MOMENTUM),\n",
    "                     metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDN3Uo9p2JuA"
   },
   "source": [
    "### Train (Fit) Base InceptionResNet & save information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "GttipXC32JuA",
    "outputId": "c3b51617-790f-43e1-cc9b-e8887bd9eebc"
   },
   "outputs": [],
   "source": [
    "# Train and save the history of training + params of model\n",
    "inceptionresnetv2_base_hist = fit(inceptionresnetv2_base, EPOCHS, TITLE, generators_299, es, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simple accuracy/loss graphs\n",
    "base_model_plot(inceptionresnetv2_base_hist, TITLE, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Better Model between ResNet50 and InceptionResnetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionresnetv2_max = np.max(inceptionresnetv2_base_hist['val_acc'])\n",
    "resnet50_max = np.max(resnet50_base_hist['val_acc'])\n",
    "\n",
    "if resnet50_max > inceptionresnetv2_max:\n",
    "    better_mod = 'resnet50'\n",
    "else:\n",
    "    better_mod = 'inceptionresnetv2'\n",
    "\n",
    "print('resnet50 max val acc: {}'.format(resnet50_max))\n",
    "print('inceptionresnetv2 max val acc: {}'.format(resnet50_max))\n",
    "print('The better model is {}'.format(better_mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qegaawmp4Mod"
   },
   "source": [
    "# InceptionResNet Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDPABaas4Rup"
   },
   "source": [
    "## Batch Size\n",
    "### Set Parameters for Batch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXOI8ydR4gOr",
    "outputId": "e3558f91-68dc-4bf8-959f-deb919ae738a"
   },
   "outputs": [],
   "source": [
    "TITLE = 'inceptionresnetv2_bs'\n",
    "BATCH_SIZE = [64, 256]\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_NEURONS = 1024\n",
    "DROPOUT_RATE = 0.5\n",
    "MOMENTUM = 0.9\n",
    "OPTIMISER = 'SGD'\n",
    "START_LAYER = 759\n",
    "LAST_LAYER = 780\n",
    "\n",
    "bs_generators = {}\n",
    "# Prepare data for base model\n",
    "for bs in BATCH_SIZE:\n",
    "    bs_generators[bs] = generate_data(train_df, val_df, test_df, IMG_SIZE_2, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-oIvqb44Moo"
   },
   "source": [
    "### Create Model for Batch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lzp8F0AP4Moo",
    "outputId": "b3fbcabd-9cf4-4464-87ee-e09533fbc79c"
   },
   "outputs": [],
   "source": [
    "inceptionresnetv2_bs_models = []\n",
    "for bs in BATCH_SIZE:\n",
    "    # Download InceptionResNet weights\n",
    "    inceptionresnetv2_conv = InceptionResNetV2(weights='imagenet', include_top=False, \n",
    "                                      input_tensor=None, input_shape=(299,299,3))\n",
    "    \n",
    "    # freeze all layers except last block\n",
    "    for layer in inceptionresnetv2_conv.layers[:START_LAYER]:\n",
    "        layer.trainable = False\n",
    "    for layer in inceptionresnetv2_conv.layers[START_LAYER:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Build model\n",
    "    model = models.Sequential()\n",
    "    model.add(inceptionresnetv2_conv)            \n",
    "    model.add(layers.Flatten())          \n",
    "    model.add(layers.Dense(NUM_NEURONS, activation='relu'))\n",
    "    model.add(layers.Dropout(DROPOUT_RATE))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # append model to list\n",
    "    inceptionresnetv2_bs_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfEecJtI4Mor"
   },
   "source": [
    "### Train (Fit) Batch Sizes Model & save information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X6Rrudkn4Mor",
    "outputId": "dd2b2a8a-d2eb-490a-d3a9-3f97ee3f036a"
   },
   "outputs": [],
   "source": [
    "# Train and save the history of training + params of model\n",
    "bs_history = {}\n",
    "\n",
    "for model, bs, gen in zip(inceptionresnetv2_bs_models, BATCH_SIZE, bs_generators):\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=SGD(lr=LEARNING_RATE, momentum=MOMENTUM),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    title = '{}{}'.format(TITLE, bs)\n",
    "    print('Running {}'.format(title))\n",
    "    bs_history[title] = fit(model, EPOCHS, title, bs_generators[gen], es, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simple accuracy/loss graphs\n",
    "for title, history in bs_history.items():\n",
    "    base_model_plot(history, title, SAVE_PATH)\n",
    "    \n",
    "# Add base model (bs128) into history dict\n",
    "key = '{}{}'.format(TITLE, 128)\n",
    "bs_history[key] = inceptionresnetv2_base_hist\n",
    "    \n",
    "# Plot combined val acc/ val loss graphs\n",
    "param_opt_plot(bs_history, TITLE, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Batch Size\n",
    "max_acc = 0\n",
    "best = None\n",
    "for name, hist in bs_history:\n",
    "    max_val_acc = np.max(hist['val_acc'])\n",
    "    if max_val_acc > max_acc:\n",
    "        max_acc = max_val_acc\n",
    "        best = name\n",
    "\n",
    "print('The optimal batch size is {} with max validation accuracy of {}'.format(best, max_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf1bXzlQ6rEg"
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhNR7efu6rEm"
   },
   "source": [
    "### Set Parameters for Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWJTQjGd6rEn"
   },
   "outputs": [],
   "source": [
    "TITLE = 'inceptionresnetv2_dropout'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_NEURONS = 1024\n",
    "DROPOUT_RATE = [0, 0.2]\n",
    "MOMENTUM = 0.9\n",
    "OPTIMISER = 'SGD'\n",
    "START_LAYER = 759\n",
    "LAST_LAYER = 780"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zK0ub21e6rEr"
   },
   "source": [
    "### Create Model for Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKEiS8fX6rEs"
   },
   "outputs": [],
   "source": [
    "inceptionresnetv2_dropout_models = []\n",
    "for dropout in DROPOUT_RATE:\n",
    "    # Download InceptionResNet weights\n",
    "    inceptionresnetv2_conv = InceptionResNetV2(weights='imagenet', include_top=False, \n",
    "                                      input_tensor=None, input_shape=(299,299,3))\n",
    "    \n",
    "    # freeze all layers except last block\n",
    "    for layer in inceptionresnetv2_conv.layers[:START_LAYER]:\n",
    "        layer.trainable = False\n",
    "    for layer in inceptionresnetv2_conv.layers[START_LAYER:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Build model\n",
    "    model = models.Sequential()\n",
    "    model.add(inceptionresnetv2_conv)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(NUM_NEURONS, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # append model to list\n",
    "    inceptionresnetv2_dropout_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPiCje8j6rEu"
   },
   "source": [
    "### Train (Fit) Dropout Model & save information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3gvc-c16rEu",
    "outputId": "10988089-9118-4e8e-f3af-908614f08b4d"
   },
   "outputs": [],
   "source": [
    "# Train and save the history of training + params of model\n",
    "dropout_history = {}\n",
    "\n",
    "for model, dropout in zip(inceptionresnetv2_dropout_models, DROPOUT_RATE):\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=SGD(lr=LEARNING_RATE, momentum=MOMENTUM),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    title = '{}{}'.format(TITLE, dropout)\n",
    "    print('Running {}'.format(title))\n",
    "    dropout_history[dropout] = fit(model, EPOCHS, title, generators_299, es, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simple accuracy/loss graphs\n",
    "for title, history in dropout_history.items():\n",
    "    base_model_plot(history, title, SAVE_PATH)\n",
    "    \n",
    "# Add base model (bs128) into history dict\n",
    "key = '{}{}'.format(TITLE, 0.5)\n",
    "dropout_history[key] = inceptionresnetv2_base_hist\n",
    "    \n",
    "# Plot combined val acc/ val loss graphs\n",
    "param_opt_plot(dropout_history, TITLE, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Dropout Rate\n",
    "max_acc = 0\n",
    "best = None\n",
    "for name, hist in dropout_history:\n",
    "    max_val_acc = np.max(hist['val_acc'])\n",
    "    if max_val_acc > max_acc:\n",
    "        max_acc = max_val_acc\n",
    "        best = name\n",
    "\n",
    "print('The optimal dropout rate is {} with max validation accuracy of {}'.format(best, max_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B42QTJlX74j4"
   },
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7T6I0Bz74j9"
   },
   "source": [
    "### Set Parameters for Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91Xf0g0L74j-"
   },
   "outputs": [],
   "source": [
    "TITLE = 'inceptionresnetv2_lr'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = [1e-2, 1e-3]\n",
    "NUM_NEURONS = 1024\n",
    "DROPOUT_RATE = 0.5\n",
    "MOMENTUM = 0.9\n",
    "OPTIMISER = 'SGD'\n",
    "START_LAYER = 759\n",
    "LAST_LAYER = 780"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7h_hBhu74kC"
   },
   "source": [
    "### Create Model for Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biVZcnuO74kD"
   },
   "outputs": [],
   "source": [
    "inceptionresnetv2_lr_models = []\n",
    "for lr in LEARNING_RATE:\n",
    "    # Download InceptionResNet weights\n",
    "    inceptionresnetv2_conv = InceptionResNetV2(weights='imagenet', include_top=False, input_tensor=None, input_shape=(299,299,3))\n",
    "    \n",
    "    # freeze all layers except last block\n",
    "    for layer in inceptionresnetv2_conv.layers[:START_LAYER]:\n",
    "        layer.trainable = False\n",
    "    for layer in inceptionresnetv2_conv.layers[START_LAYER:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    # Build model\n",
    "    model = models.Sequential()\n",
    "    model.add(inceptionresnetv2_conv)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(NUM_NEURONS, activation='relu'))\n",
    "    model.add(layers.Dropout(DROPOUT_RATE))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # append model to list\n",
    "    inceptionresnetv2_lr_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZRB8ePY74kF"
   },
   "source": [
    "### Train (Fit) Dropout Model & save information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wapd2NXO74kG",
    "outputId": "10988089-9118-4e8e-f3af-908614f08b4d"
   },
   "outputs": [],
   "source": [
    "# Train and save the history of training + params of model\n",
    "lr_history = {}\n",
    "\n",
    "for model, lr in zip(inceptionresnetv2_lr_models, LEARNING_RATE):\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=SGD(lr=lr, momentum=MOMENTUM),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    title = '{}{}'.format(TITLE, lr)\n",
    "    print('Running {}'.format(title))\n",
    "    lr_history[lr] = fit(model, EPOCHS, title, generators_299, es, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simple accuracy/loss graphs\n",
    "for title, history in lr_history.items():\n",
    "    base_model_plot(history, title, SAVE_PATH)\n",
    "    \n",
    "# Add base model (bs128) into history dict\n",
    "key = '{}{}'.format(TITLE, 1e-4)\n",
    "lr_history[key] = inceptionresnetv2_base_hist\n",
    "    \n",
    "# Plot combined val acc/ val loss graphs\n",
    "param_opt_plot(lr_history, TITLE, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Learning Rate\n",
    "max_acc = 0\n",
    "best = None\n",
    "for name, hist in lr_history:\n",
    "    max_val_acc = np.max(hist['val_acc'])\n",
    "    if max_val_acc > max_acc:\n",
    "        max_acc = max_val_acc\n",
    "        best = name\n",
    "\n",
    "print('The optimal learning rate is {} with max validation accuracy of {}'.format(best, max_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Om7jtNud89UU"
   },
   "source": [
    "## Stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCWV1sZr89Ua"
   },
   "source": [
    "### Set Parameters for Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYnkzu2w89Ua"
   },
   "outputs": [],
   "source": [
    "TITLE = 'inceptionresnetv2_stacked'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_NEURONS = 1024\n",
    "DROPOUT_RATE = 0.5\n",
    "MOMENTUM = 0.9\n",
    "OPTIMISER = 'SGD'\n",
    "START_LAYER = 759\n",
    "LAST_LAYER = 780"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_2cj8mq89Ue"
   },
   "source": [
    "### Create Model for Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIflRqWQ89Uf"
   },
   "outputs": [],
   "source": [
    "# Download InceptionResNet weights\n",
    "inceptionresnetv2_conv = InceptionResNetV2(weights='imagenet', include_top=False, input_tensor=None, input_shape=(299,299,3))\n",
    "\n",
    "# freeze all layers except last block\n",
    "for layer in inceptionresnetv2_conv.layers[:START_LAYER]:\n",
    "    layer.trainable = False\n",
    "for layer in inceptionresnetv2_conv.layers[START_LAYER:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Build model\n",
    "stacked_model = models.Sequential()\n",
    "stacked_model.add(inceptionresnetv2_conv)\n",
    "stacked_model.add(layers.Flatten())\n",
    "stacked_model.add(layers.Dense(NUM_NEURONS, activation='relu'))\n",
    "stacked_model.add(layers.Dropout(DROPOUT_RATE))\n",
    "stacked_model.add(layers.Dense(NUM_NEURONS, activation='relu'))\n",
    "stacked_model.add(layers.Dropout(DROPOUT_RATE))\n",
    "stacked_model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfofnOl489Uh"
   },
   "source": [
    "### Train (Fit) Stacked Model & save information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mmkgqLk89Ui",
    "outputId": "10988089-9118-4e8e-f3af-908614f08b4d"
   },
   "outputs": [],
   "source": [
    "# Train and save the history of training + params of model\n",
    "stacked_model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=SGD(lr=LEARNING_RATE, momentum=MOMENTUM),\n",
    "                      metrics=['acc'])\n",
    "\n",
    "stacked_history = {}\n",
    "stacked_history['inceptionresnetv2_stacked'] = fit(stacked_model, EPOCHS, TITLE, generators_299, es, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simple accuracy/loss graphs\n",
    "for title, history in stacked_history.items():\n",
    "    base_model_plot(history, title, SAVE_PATH)\n",
    "    \n",
    "# Add base model (bs128) into history dict\n",
    "key = 'inceptionresnetv2_unstacked'\n",
    "stacked_history[key] = inceptionresnetv2_base_hist\n",
    "    \n",
    "# Plot combined val acc/ val loss graphs\n",
    "param_opt_plot(stacked_history, 'inceptionresnetv2_layered', SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Number of Layers\n",
    "max_acc = 0\n",
    "best = None\n",
    "for name, hist in stacked_history:\n",
    "    max_val_acc = np.max(hist['val_acc'])\n",
    "    if max_val_acc > max_acc:\n",
    "        max_acc = max_val_acc\n",
    "        best = name\n",
    "\n",
    "print('The optimal number of layers is {} with max validation accuracy of {}'.format(best, max_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save my optimal model:\n",
    "Based on results from previous parts, optimal model is:\n",
    "1. Batch Size = 128\n",
    "2. Dropout Rate = 0.5\n",
    "3. Learning Rate = 0.01\n",
    "4. Number of Fully-Connected Layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_history = lr_history[1e-2]\n",
    "save_history(optimal_history, SAVE_PATH, 'inceptionresnetv2_optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all 6 base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all my plots in this folder and then can run together\n",
    "directory = './results/histories'\n",
    "\n",
    "base_histories = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if 'base' in filename:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        print('Found file at: {}'.format(filepath))\n",
    "\n",
    "        with open(filename) as json_file:\n",
    "            base_histories[filename] = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot combined val acc/ val loss graphs\n",
    "param_opt_plot(base_histories, 'base_models', SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all optimal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all my plots in this folder and then can run together\n",
    "directory = './results/histories'\n",
    "\n",
    "optimal_histories = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if 'optimal' in filename:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        print('Found file at: {}'.format(filepath))\n",
    "\n",
    "        with open(filename) as json_file:\n",
    "            optimal_histories[filename] = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot combined val acc/ val loss graphs\n",
    "param_opt_plot(optimal_histories, 'optimal_models', SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Transfer Learning Models\n",
    "\n",
    "[Nithya] I'm not sure how you are going to name them but you can either\n",
    "1. Name them in a way that there is a word that only models in this part has then read in everything + the optimal model OR\n",
    "2. Call the param_opt_plot function directly after your finetuning if you have a dictionary containing the 4 models\n",
    " (unfreezeAll, unfreeze1Top, unfreeze2Top and optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all my plots in this folder and then can run together\n",
    "directory = './results/histories'\n",
    "\n",
    "tl_histories = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if 'tl' in filename or 'vgg16_optimal' in filename:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        print('Found file at: {}'.format(filepath))\n",
    "\n",
    "        with open(filename) as json_file:\n",
    "            tl_histories[filename] = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot combined val acc/ val loss graphs\n",
    "param_opt_plot(tl_histories, 'tl_models', SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the maximum validation accuracy for all models trained in project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ4FElTaiXnM",
    "outputId": "47bf1b53-9cc8-4070-e01d-da68dcf200ec"
   },
   "outputs": [],
   "source": [
    "# Plot Max Validation Accuracies\n",
    "all_histories = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    print('Found file at: {}'.format(filepath))\n",
    "    with open(filename) as json_file:\n",
    "        all_histories[filename] = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print max val acc results and epoch and save to a dictionary\n",
    "max_val_acc = {}\n",
    "overall_max = 0\n",
    "max_model = None\n",
    "print('Max Validation Accuracy for each model')\n",
    "for model, hist in all_histories.items():\n",
    "    if 'acc' in hist.keys():\n",
    "        max_val_acc[model] = np.max(hist['val_acc'])\n",
    "        epoch = np.argmax(history['val_acc']) + 1\n",
    "    else:\n",
    "        max_val_acc[model] = np.max(hist['val_accuracy'])\n",
    "        epoch = np.argmax(hist['val_accuracy']) + 1\n",
    "    \n",
    "    print('{:<25}: {:.4f}'.format(model, max_val_acc[model]))\n",
    "    if max_val_acc[model] > overall_max:\n",
    "        overall_max = max_val_acc[model]\n",
    "        max_model = model\n",
    "print()\n",
    "print('The best model is {} with {:.4f}'.format(max_model, overall_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe and save as csv file\n",
    "df = pd.DataFrame(list(max_val_acc.items()),columns = ['model', 'max_val_acc']) \n",
    "df_sorted = df.sort_values(by=['max_val_acc'])\n",
    "df_sorted.to_csv('./results/all_max_val_acc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jDIkn8tC0l3k",
    "cgrhIfr90l37",
    "_GCR4HzB0l4I",
    "B42QTJlX74j4"
   ],
   "name": "wanqi_resnet50_inceptionresnet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
