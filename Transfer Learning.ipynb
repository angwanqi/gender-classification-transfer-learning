{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Set Random Seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section imports all packages required for the context of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import json\n",
    "import pickle\n",
    "import random as python_random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import ResNet50, InceptionResNetV2, VGG16, InceptionV3, DenseNet201\n",
    "from keras import models, layers\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seeds for reproducible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional checks done to ensure required folders are already created, if not create it. The following folders will be checked.\n",
    "\n",
    "1. `./weights`: Store the optimal weights of every model trained, named as `<model_name>.h5`\n",
    "2. `./results`: Store the visualisation of each models performance\n",
    "    * `./results/acc_loss_plots`: Store the loss and accuracy curves of each model's training\n",
    "    * `./results/metrics`: Store the f1 and auc score comparison bar graphs\n",
    "    * `./results/cm`: Store the confusion matrix generated by each model upon the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./weights'):\n",
    "    os.mkdir('./weights')\n",
    "\n",
    "if not os.path.exists('./results'):\n",
    "    os.mkdir('./results')\n",
    "\n",
    "subfolders = ['acc_loss_plots', 'clfr', 'cm', 'metrics']\n",
    "for sf in subfolders:\n",
    "    filepath = './results/{}'.format(sf)\n",
    "    if not os.path.exists(filepath):\n",
    "        os.mkdir(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used, the IMDB-WIKI dataset faces can be downloaded using the following terminal command and preprocessed using the `mat.py` file which was obtained from the following repository: https://github.com/imdeepmind/processed-imdb-wiki-dataset/blob/master/mat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run mat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section loads the dataset, splits the dataset into Training, Testing and Validation and loads these split datasets into separate `ImageDataGenerator`.\n",
    "\n",
    "Additionally, existing results from previous sessions are loaded and saved for ease of collaboration and evaluation in the future. \n",
    "\n",
    "  Model histories are saved into a combined `histories.pickle` file as a dictionary with the following structure:\n",
    "{\n",
    "    `model_name`: Keras `history` callback object\n",
    "}. \n",
    "\n",
    "  The maximum validation accuracy, F1 score and AUC_ROC score are saved into the `model_performance.csv` file in the following format: <`model_name`, `max_val_acc`, `f1_score`, `roc_auc`>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('meta.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution is explored to examine if any additional steps is required to ensure a balanced class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['gender'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this is an imbalanced dataset and hence requires a stratified split into training, testing and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First take out 20% for test (this dataset will not be used in the training process)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=0, stratify=df['gender'])\n",
    "# Then split the remaining 80% into training and validation\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=0, stratify=train_df['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, Validation and Testing geneerators are defined. In addition, a `predict_generator` is added on which model evaluation of generating the F1 and ROC_AUC scores is performed. This additional generator is required so that a comparison can be made by setting the parameter `shuffle=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(img_size=224, batch_size=128):\n",
    "  # data generator for training data\n",
    "  train_generator = train_datagen.flow_from_dataframe(train_df, x_col=\"path\", \n",
    "                                                      y_col=\"gender\", \n",
    "                                                      target_size=(img_size,img_size), \n",
    "                                                      batch_size=batch_size, \n",
    "                                                      class_mode='binary')\n",
    "\n",
    "  # data generator for validation data\n",
    "  validation_generator = validation_datagen.flow_from_dataframe(val_df, x_col=\"path\", \n",
    "                                                                y_col=\"gender\", \n",
    "                                                                target_size=(img_size,img_size),\n",
    "                                                                batch_size=batch_size, \n",
    "                                                                class_mode='binary')\n",
    "\n",
    "  # data generator for validation data to be used for prediction\n",
    "  predict_generator = validation_datagen.flow_from_dataframe(val_df, x_col=\"path\", \n",
    "                                                             y_col=\"gender\", \n",
    "                                                             target_size=(img_size,img_size),\n",
    "                                                             batch_size=batch_size, \n",
    "                                                             class_mode='binary',\n",
    "                                                             shuffle = False)\n",
    "\n",
    "  # data generator for testing data\n",
    "  test_generator = test_datagen.flow_from_dataframe(test_df, x_col=\"path\", \n",
    "                                                    y_col=\"gender\", \n",
    "                                                    target_size=(img_size,img_size),\n",
    "                                                    batch_size=batch_size, \n",
    "                                                    class_mode='binary',\n",
    "                                                    shuffle = False)\n",
    "  \n",
    "  generators = {'train_gen': train_generator,\n",
    "                'validation_gen': validation_generator,\n",
    "                'test_gen': test_generator,\n",
    "                'predict_gen': predict_generator}\n",
    "  \n",
    "  return generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the most updated `histories` object and explore which models have been fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history():\n",
    "    hist_out_file = open(\"histories.pickle\", \"wb\")\n",
    "    pickle.dump(histories, hist_out_file)\n",
    "    hist_out_file.close()\n",
    "\n",
    "def open_history():\n",
    "    try:\n",
    "        file = open(\"histories.pickle\", 'rb')\n",
    "        pickleData = pickle.load(file)\n",
    "        file.close()\n",
    "    except (OSError, IOError) as e:\n",
    "        pickleData = {}\n",
    "        pickle.dump(pickleData, open(\"histories.pickle\", \"wb\"))\n",
    "    return pickleData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = open_history()\n",
    "histories.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the most updated `performance_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_performance():\n",
    "    if os.path.exists('model_performance.csv'):\n",
    "        return pd.read_csv(\"model_performance.csv\", index_col=False)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['model_name', 'max_val_acc', 'error', 'f1_score', 'roc_auc'])\n",
    "        \n",
    "def save_performance(model_name, error, f1, roc_auc):\n",
    "    global performance_df\n",
    "    new_row = {\n",
    "        'model_name': model_name,\n",
    "        'max_val_acc': np.amax(histories[model_name][\"val_acc\"]),\n",
    "        'error': error,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    performance_df = performance_df.append(new_row, ignore_index=True)\n",
    "    performance_df.to_csv('model_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = open_performance()\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions play a part in the training of a model. \n",
    "\n",
    "The callbacks `ModelCheckpoint` and `EarlyStopping` are implemented during the training of the model to stop model training before overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc(title):\n",
    "    return ModelCheckpoint('./weights/{}.h5'.format(title), \n",
    "                           monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', min_delta=0.001, verbose=1, patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all models follow a similar baseleine, a baseline build function `build_model()` is defined with the hyperparameters optimised as default parameters to the function. A general `fit()` function is also called, which trains the model and saves the model history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(tl_model, lr=1e-4, drop_rate=0.5, stacked=False):\n",
    "    model = models.Sequential()\n",
    "    model.add(tl_model)      \n",
    "    model.add(layers.Flatten())\n",
    "    if stacked:\n",
    "        model.add(layers.Dense(1024, activation='relu'))\n",
    "        if drop_rate > 0:\n",
    "            model.add(layers.Dropout(drop_rate))\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    if drop_rate > 0:\n",
    "        model.add(layers.Dropout(drop_rate))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=SGD(lr=lr, momentum=0.9),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, title, generator):\n",
    "    train_gen = generator['train_gen']\n",
    "    validation_gen = generator['validation_gen']\n",
    "    \n",
    "    history = model.fit(train_gen, \n",
    "                      steps_per_epoch=train_gen.samples/train_gen.batch_size, \n",
    "                      epochs=2,\n",
    "                      validation_data=validation_gen,\n",
    "                      validation_steps=validation_gen.samples/validation_gen.batch_size,\n",
    "                      verbose=2,\n",
    "                      callbacks=[es, mc(title)])\n",
    "    \n",
    "    histories[title] = history.history\n",
    "    save_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used for model evaluation:\n",
    "1. `plot_curves()` plot the training and validation curves based on the model's training history. `compare_curves()` is a similar method but plots the training histories of multiple models, as defined by `model_names`, on the same plot.\n",
    "2. `model_analysis()` performs a complete analysis on the performance of the model on the validation dataset. This includes computing the error_rate on the validation dataset, f1_score, roc_auc_score. For greater analysis, the confusion matrix is also plotted alongside a comprehensive classification_report which is generated and saved.\n",
    "3. `compare_metrics()` visualises the performance of accuracy, f1 and auc scores of multiple models, as defined by `model_names`, on the same plot through bar graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(model_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    h = histories[model_name]\n",
    "\n",
    "    ax1.plot(h[\"loss\"], label=\"Training\")\n",
    "    ax1.plot(h[\"val_loss\"], label=\"Validation\")\n",
    "    ax1.set_xlabel(\"# Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"best\")\n",
    "\n",
    "    ax2.plot(h[\"acc\"], label=\"Training\")\n",
    "    ax2.plot(h[\"val_acc\"], label=\"Validation\")\n",
    "    ax2.set_xlabel(\"# Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "\n",
    "    fig.suptitle(\"{} Training History\".format(model_name))\n",
    "    plt.savefig(\"results/acc_loss_plots/{}_Curves.png\".format(model_name))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def compare_curves(model_names, labels, title):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    for i, name in enumerate(model_names):\n",
    "        h = histories[name]\n",
    "        ax1.plot(h[\"val_loss\"], label=labels[i])\n",
    "        ax2.plot(h[\"val_acc\"], label=labels[i])\n",
    "\n",
    "    ax1.set_xlabel(\"# Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"best\")\n",
    "    \n",
    "    ax2.set_xlabel(\"# Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    \n",
    "    fig.suptitle(\"{} Training Comparison\".format(title))\n",
    "    plt.savefig(\"results/acc_loss_plots/{}_Comparison.png\".format(title))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis(model, title, pred_gen):\n",
    "    print('Results for {}'.format(title))\n",
    "    prediction = model.predict(pred_gen,\n",
    "                             steps=pred_gen.samples/pred_gen.batch_size,\n",
    "                             verbose=2)\n",
    "    predicted_classes = prediction.flatten()\n",
    "    # Threshold output\n",
    "    predicted_classes[predicted_classes>=0.5] = 1\n",
    "    predicted_classes[predicted_classes<0.5] = 0\n",
    "\n",
    "    actual = pred_gen.classes\n",
    "    errors = np.where(predicted_classes != actual)[0]\n",
    "    error_rate = len(errors)/pred_gen.samples\n",
    "    print(\"Error rate {}\".format(error_rate))\n",
    "    \n",
    "    # Geenerate confusion matrix\n",
    "    genders = ['Female', 'Male']\n",
    "    cm = confusion_matrix(actual, predicted_classes)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt=\"d\",\n",
    "              xticklabels=genders, yticklabels=genders)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('results/cm/{}_cm.png'.format(title))\n",
    "    plt.show()\n",
    "\n",
    "    # Generate classification report\n",
    "    print(classification_report(actual, predicted_classes, target_names=genders))\n",
    "    clsf_report = pd.DataFrame(classification_report(actual, predicted_classes, target_names=genders, output_dict=True)).transpose()\n",
    "    clsf_report.to_csv('results/clfr/{}_cr.csv'.format(title), index= True)\n",
    "\n",
    "    # Compile all metrics and add to csv\n",
    "    f1 = f1_score(actual, predicted_classes, average='weighted')\n",
    "    roc_auc = roc_auc_score(actual, predicted_classes)\n",
    "    save_performance(title, error_rate, f1, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics(model_names, labels, title):\n",
    "    subset_df = performance_df.loc[performance_df['model_name'].isin(model_names)]\n",
    "    subset_df.plot(x=\"model_name\", y=[\"max_val_acc\", \"f1_score\", \"roc_auc\"], kind=\"bar\")\n",
    "    \n",
    "    val = subset_df.loc[:, [\"max_val_acc\", \"f1_score\", \"roc_auc\"]].values\n",
    "    plt.gca().set_ylim(bottom=np.amin(val)-0.005, top=np.amax(val)+0.001)\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    plt.title(\"{} Comparison\".format(title))\n",
    "    plt.savefig(\"results/metrics/{}_Comparison.png\".format(title))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the various architectures are instantiated with everything except the last block of each transfer learning model is frozen using `layer.trainable = False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16\n",
    "\n",
    "VGG16 takes in an input image size of 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_net = VGG16(weights='imagenet', include_top=False, input_tensor=None, input_shape=(224,224,3))\n",
    "vgg16_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg16_net.layers[:-5]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vgg16_net, lr=1e-3)\n",
    "fit(model, \"vgg16\", generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"vgg16\")\n",
    "model_analysis(model, \"vgg16\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionV3\n",
    "\n",
    "InceptionV3 takes in an input image size of 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_net = InceptionV3(weights='imagenet', include_top=False, input_tensor=None, input_shape=(224,224,3))\n",
    "inception_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in inception_net.layers[:-22]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(inception_net)\n",
    "fit(model, \"inceptionV3\", generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"inceptionV3\")\n",
    "model_analysis(model, \"inceptionV3\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50\n",
    "\n",
    "ResNet50 takes in an input image size of 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_conv = ResNet50(weights='imagenet', include_top=False, input_tensor=None, input_shape=(224,224,3))\n",
    "resnet50_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in resnet50_conv.layers[:143]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(resnet50_conv)\n",
    "fit(model, \"resnet50\", generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"resnet50\")\n",
    "model_analysis(model, \"resnet50\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception\n",
    "\n",
    "Xception takes in an input image size of 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_model = Xception(weights='imagenet', include_top=False, input_tensor=None, input_shape=(224,224,3))\n",
    "xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in xception_model.layers[:-6]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(xception_model)\n",
    "fit(model, \"xception\", generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"xception\")\n",
    "model_analysis(model, \"xception\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionResnetV2\n",
    "\n",
    "Xception takes in an input image size of 229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = generate_data(img_size=229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionresnetv2_conv = InceptionResNetV2(weights='imagenet', include_top=False, input_tensor=None, input_shape=(299,299,3))\n",
    "inceptionresnetv2_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in inceptionresnetv2_conv.layers[:759]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(inceptionresnetv2_conv)\n",
    "fit(model, \"IncResV2\", generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"IncResV2\")\n",
    "model_analysis(model, \"IncResV2\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet201\n",
    "\n",
    "Xception takes in an input image size of 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseNet_model = DenseNet201(weights='imagenet', include_top=False, input_tensor=None, input_shape=(224,224,3))\n",
    "denseNet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in denseNet_model.layers[:-9]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(denseNet_model)\n",
    "fit(model, \"densenet\", generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"densenet\")\n",
    "model_analysis(model, \"densenet\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Comparison across Base Models\n",
    "\n",
    "Plot the comparison curves across the 6 base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"vgg16\", \"inceptionV3\", \"resnet50\", \"xception\", \"IncResV2\", \"densenet\"]\n",
    "compare_curves(model_names, model_names, \"Base_Comparison\")\n",
    "compare_metrics(model_names, model_names, \"Base_Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimisation\n",
    "\n",
    "The 3 best models - VGG16, InceptionResnetV2 and DenseNet, are chosen and will have their parameters (batch size, dropout rate, learning rate and presence of stacked fully connected layers). Hence, the following functions were created to prevent any duplication of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMAL_ARCH = {\n",
    "    \"vgg16\": 224,\n",
    "    \"IncResV2\": 229,\n",
    "    \"densenet\": 224\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tlModel(arch):\n",
    "    if arch == \"vgg16\":\n",
    "        tl_model = VGG16(weights='imagenet', include_top=False, input_tensor=None, input_shape=(224,224,3))\n",
    "        for layer in tl_model.layers[:-5]:\n",
    "            layer.trainable = False\n",
    "    elif arch == \"IncResV2\":\n",
    "        tl_model = inceptionresnetv2_conv = InceptionResNetV2(weights='imagenet', include_top=False, input_tensor=None, input_shape=(299,299,3))\n",
    "        for layer in tl_model.layers[:759]:\n",
    "            layer.trainable = False\n",
    "    elif arch == \"densenet\":\n",
    "        tl_model = DenseNet201(weights='imagenet', include_top=False, input_tensor=None, input_shape=(224,224,3))\n",
    "        for layer in tl_model.layers[:-9]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        print(\"model not found\")\n",
    "        tl_model = None\n",
    "    return tl_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise Batch Size\n",
    "\n",
    "3 different batch sizes are considered - 64, 128 (base model) and 256. Therefore, each of the three architectures are then rebuilt and trained with the 2 different batch sizes - through the different image generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = [64, 256]\n",
    "labels = [\"bs-128\", \"bs-64\", \"bs-256\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch, img_size in OPTIMAL_ARCH.items():\n",
    "    for bs in BATCH_SIZES:\n",
    "        generators = generate_data(img_size=img_size, batch_size=bs)\n",
    "        model_name = \"{}_bs-{}\".format(arch, bs)\n",
    "        lr = 1e-3 if arch == \"vgg16\" else 1e-4\n",
    "        model = build_model(create_tlModel(arch), lr=lr)\n",
    "        fit(model, model_name, generators)\n",
    "        model_analysis(model, model_name, generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in OPTIMAL_ARCH.keys():\n",
    "    model_names = [arch]\n",
    "    for bs in BATCH_SIZES:\n",
    "        model_names.append(\"{}_bs-{}\".format(arch, bs))\n",
    "    compare_curves(model_names, labels, \"{}_bs-Comparison\".format(arch))\n",
    "    compare_metrics(model_names, labels, \"{}_bs-Comparison\".format(arch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Rate\n",
    "\n",
    "3 different dropout values are considered - 0, 0.2 and 0.5 (base model). Therefore each of the three architectures are then rebuilt and trained with the 2 different dropout values - as defined by the parameter `drop_rate` in the `build_model()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_RATES = [0, 0.2]\n",
    "labels = [\"dr-0.5\", \"dr-0\", \"dr-0.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch, img_size in OPTIMAL_ARCH.items():\n",
    "    generators = generate_data(img_size=img_size)\n",
    "    for dr in DROP_RATES:\n",
    "        model_name = \"{}_dr-{}\".format(arch, dr)\n",
    "        lr = 1e-3 if arch == \"vgg16\" else 1e-4\n",
    "        model = build_model(create_tlModel(arch), lr=lr, drop_rate=dr)\n",
    "        fit(model, model_name, generators)\n",
    "        model_analysis(model, model_name, generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in OPTIMAL_ARCH.keys():\n",
    "    model_names = [arch]\n",
    "    for dr in DROP_RATES:\n",
    "        model_names.append(\"{}_dr-{}\".format(arch, dr))\n",
    "    compare_curves(model_names, labels, \"{}_dr-Comparison\".format(arch))\n",
    "    compare_metrics(model_names, labels, \"{}_dr-Comparison\".format(arch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise Learning Rate\n",
    "\n",
    "3 different learning rates are considered - 0.01, 0.001 and 0.0001 (base model). Therefore each of the three architectures are then rebuilt and trained with the 2 different learning rates - as defined by the parameter `lr` in the `build_model()` function. The below code is slightly less straightford since the base vgg16 model differs from the rest of the model, starting with a higher learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATES = [1e-2, 1e-3]\n",
    "labels = [\"lr-0.0001\", \"lr-0.01\", \"lr-0.001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch, img_size in OPTIMAL_ARCH.items():\n",
    "    generators = generate_data(img_size=img_size)\n",
    "    for lr in LEARNING_RATES:\n",
    "        if arch == \"vgg16\" and lr == 1e-3:\n",
    "            model_name = \"vgg16_lr-0.0001\"\n",
    "            model = build_model(create_tlModel(arch), lr=1e-4)\n",
    "        else:\n",
    "            model_name = \"{}_lr-{}\".format(arch, lr)\n",
    "            model = build_model(create_tlModel(arch), lr=lr)\n",
    "        fit(model, model_name, generators)\n",
    "        model_analysis(model, model_name, generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in OPTIMAL_ARCH.keys():\n",
    "    model_names = [arch]\n",
    "    if arch !== \"vgg16\":\n",
    "        for lr in LEARNING_RATES:\n",
    "            model_names.append(\"{}_lr-{}\".format(arch, lr))\n",
    "        compare_curves(model_names, labels, \"{}_lr-Comparison\".format(arch))\n",
    "        compare_metrics(model_names, labels, \"{}_lr-Comparison\".format(arch))\n",
    "    else:\n",
    "        model_names.extend([\"vgg16_lr-0.01\", \"vgg16_lr-0.0001\"])\n",
    "        compare_curves(model_names, [\"lr-0.001\", \"lr-0.01\", \"lr-0.0001\"], \"vgg16_lr-Comparison\")\n",
    "        compare_metrics(model_names, [\"lr-0.001\", \"lr-0.01\", \"lr-0.0001\"], \"vgg16_lr-Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise Stacking\n",
    "\n",
    "More fully connected layers are optionally added to the base model. Each of the three architectures are then rebuilt and trained with the stacking - as defined by the parameter `stacked` in the `build_model()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"unstacked, stacked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch, img_size in OPTIMAL_ARCH.items():\n",
    "    generators = generate_data(img_size=img_size)\n",
    "    model_name = \"{}_stacked\".format(arch)\n",
    "    lr = 1e-3 if arch == \"vgg16\" else 1e-4\n",
    "    model = build_model(create_tlModel(arch), lr=lr, stacked=True)\n",
    "    fit(model, model_name, generators)\n",
    "    model_analysis(model, model_name, generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in OPTIMAL_ARCH.keys():\n",
    "    model_names = [arch]\n",
    "    model_names.append(\"{}_stacked\".format(arch))\n",
    "    compare_curves(model_names, labels, \"{}_stacked-Comparison\".format(arch))\n",
    "    compare_metrics(model_names, labels, \"{}_stacked-Comparison\".format(arch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Optimal Hyperparameters\n",
    "\n",
    "In this stage we combine all ideal hyperparameters together to generate the optimal model for this architecture. It is then compared with its base model and each other.\n",
    "\n",
    "The optimal parameters for each model are as follows:\n",
    "\n",
    "|          | batch_size | learning_rate | dropout_rate | is_stacked |\n",
    "|:--------:|:----------:|:-------------:|:------------:|:----------:|\n",
    "| vgg16    | 64         | 1e-3          | 0.5          | True       |\n",
    "| IncResV2 | 128        | 1e-2          | 0.5          | False      |\n",
    "| densenet | 128        | 1e-4          | 0.5          | True       |\n",
    "\n",
    "Since only 1 parameter has changed for both ResNet50 and DenseNet201 architectures, the respective already trained models - `IncResV2_lr-0.01` and `densenet_stacked` is used and a new model is created for vgg16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal VGG16\n",
    "generators = generate_data(batch_size=64)\n",
    "model = build_model(create_tlModel(\"vgg16\"), lr=1e-3, stacked=True)\n",
    "fit(model, \"vgg16_optimal\", generators)\n",
    "model_analysis(model, \"vgg16_optimal\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"vgg16_optimal\", \"IncResV2_lr-0.01\", \"densenet_stacked\"]\n",
    "compare_curves(model_names, OPTIMAL_ARCH.keys(), \"optimal_Comprison\")\n",
    "compare_metrics(model_names, OPTIMAL_ARCH.keys(), \"optimal_Comprison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Models\n",
    "\n",
    "In this section, we build open our findings of the vgg16 model to further finetune the pretrained transfer learning portion of our model to achieve incrememtal improvements with a signifantly lower learning rate (1e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = generate_data(batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment is to unfreeze all layers and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"./weights/vgg16_optimal.h5\")\n",
    "for layer in model.layers[0].layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "print(\"Total trainable weights: {}\".format(len(model.trainable_weights)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, \"vgg16_unfreezeAll\", generators)\n",
    "model_analysis(model, \"vgg16_unfreezeAll\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second experiment includes freezing back the last block and unfreezing the first block of the preetrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"./weights/vgg16_optimal.h5\")\n",
    "for layer in model.layers[0].layers[1:4]:\n",
    "    layer.trainable = True\n",
    "for layer in model.layers[0].layers[-5:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Total trainable weights: {}\".format(len(model.trainable_weights)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, \"vgg16_unfreezeTop\", generators)\n",
    "model_analysis(model, \"vgg16_unfreezeTop\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last experiment includes unfreezing the first block of the preetrained model with the last block still unfrozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"./weights/vgg16_optimal.h5\")\n",
    "for layer in model.layers[0].layers[1:4]:\n",
    "    layer.trainable = True\n",
    "\n",
    "print(\"Total trainable weights: {}\".format(len(model.trainable_weights)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, \"vgg16_unfreezeTopBottom\", generators)\n",
    "model_analysis(model, \"vgg16_unfreezeTopBottom\", generators[\"predict_gen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models are then compared with the original base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"vgg16_optimal\", \"vgg16_unfreezeAll\", \"vgg16_unfreezeTop\", \"vgg16_unfreezeTopBottom\"]\n",
    "compare_curves(model_names, model_names, \"finetuning_Comparison\")\n",
    "compare_metrics(model_names, model_names, \"finetuning_Comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Best Model on Test Set\n",
    "\n",
    "This final stage evaluates the performance of the best model `vgg16_optimal` on the unseen test set. A complete model analysis is performed and finally, a few samples which the model has gotten correct and wrong for each class are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_dataframe(test_df,\n",
    "                                                  x_col=\"path\", y_col=\"gender\", \n",
    "                                                  target_size=(224,224), batch_size=64, \n",
    "                                                  class_mode='binary', shuffle=False)\n",
    "\n",
    "model = load_model(\"./weights/vgg16_optimal.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analysis(model, \"vgg16_optimal_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the predictions for each sample. Since a sigmoid activation was used, thresholding is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_generator,\n",
    "                            steps=test_generator.samples/test_generator.batch_size,\n",
    "                            verbose=1)\n",
    "predicted_classes = prediction.flatten()\n",
    "predicted_classes[predicted_classes>=0.5] = 1\n",
    "predicted_classes[predicted_classes<0.5] = 0\n",
    "\n",
    "actual = test_generator.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the predicted classes with the actual classes, and the indice of the samples are split based on if it is correctly classified or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorInd = np.where(predicted_classes != actual)[0]\n",
    "correctInd = np.where(predicted_classes == actual)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 4 by 5 matrix of images are plotted where the four rows represent - males classified as male, female classified as female, male classified as female, female classified as male. For each row, 5 sample images are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns, rows = 5, 4\n",
    "fig, big_axes = plt.subplots( figsize=(10, 10) , nrows=rows, ncols=1, sharey=True)\n",
    "\n",
    "row_names = [\"Male Correct\", \"Female Correct\", \"Male Wrong\", \"Female Wrong\"]\n",
    "for r, big_ax in enumerate(big_axes, start=1):\n",
    "    big_ax.set_title(row_names[r-1])\n",
    "    # Turn off axis lines and ticks of the big subplot \n",
    "    # obs alpha is 0 in RGBA string!\n",
    "    big_ax.tick_params(labelcolor=(1.,1.,1., 0.0), top='off', bottom='off', left='off', right='off')\n",
    "    # removes the white frame\n",
    "    big_ax._frameon = False\n",
    "\n",
    "i = 0 # Ind to iterate through the split indexes\n",
    "fem, male = 0, 0 # Counts to ensure that only 5 samples of each gender are plotted\n",
    "while fem < columns or male < columns:\n",
    "    if test_df.iloc[correctInd[i]][\"gender\"] == \"male\" and male < columns:\n",
    "        img = mpimg.imread(test_df.iloc[correctInd[i]][\"path\"])\n",
    "        ax = fig.add_subplot(rows,columns,male+1) # Add to the first row\n",
    "        ax.imshow(img)\n",
    "        ax.set_axis_off()\n",
    "        male += 1\n",
    "    if test_df.iloc[correctInd[i]][\"gender\"] == \"female\" and fem < columns:\n",
    "        img = mpimg.imread(test_df.iloc[correctInd[i]][\"path\"])\n",
    "        ax = fig.add_subplot(rows,columns,fem+columns+1) # Add to the second row (unraveled indexing)\n",
    "        ax.imshow(img)\n",
    "        ax.set_axis_off()\n",
    "        fem += 1  \n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "fem, male = 0, 0 \n",
    "while fem < columns or male < columns:\n",
    "    if test_df.iloc[errorInd[i]][\"gender\"] == \"male\" and male < columns:\n",
    "        img = mpimg.imread(test_df.iloc[errorInd[i]][\"path\"])\n",
    "        ax = fig.add_subplot(rows,columns,male+columns*2+1) # Add to the third row\n",
    "        ax.imshow(img)\n",
    "        ax.set_axis_off()\n",
    "        male += 1\n",
    "    if test_df.iloc[errorInd[i]][\"gender\"] == \"female\" and fem < columns:\n",
    "        img = mpimg.imread(test_df.iloc[errorInd[i]][\"path\"])\n",
    "        ax = fig.add_subplot(rows,columns,fem+columns*3+1) # Add to the fourth row\n",
    "        ax.imshow(img)\n",
    "        ax.set_axis_off()\n",
    "        fem += 1  \n",
    "    i += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
